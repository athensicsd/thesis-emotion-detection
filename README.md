Η παρούσα μελέτη εστιάζει στην ανίχνευση των συναισθημάτων που προκαλούνται κατά την παρακολούθηση έργων τέχνης. 
Συγκεκριμένα, επιδιώκει να απαντήσει στα εξής ερευνητικά ερωτήματα:

1) Ποια συναισθήματα προκαλούνται κατά τη θέαση δέκα συγκεκριμένων έργων ζωγραφικής;
2) Με ποιες μεθόδους μπορούν να συλλεχθούν και να αναλυθούν δεδομένα για την ανίχνευση των συναισθημάτων αυτών;

Για την απάντηση του δεύτερου ερωτήματος, 23 εθελοντές συμπλήρωσαν αρχικά τέσσερα ερωτηματολόγια που αφορούσαν δημογραφικά στοιχεία και τεστ προσωπικότητας (Erotimatologia.pdf).
Στη συνέχεια, σχεδιάστηκε και υλοποιήθηκε πειραματική εφαρμογή σε γλώσσα Python (art_emotion_detection.py), μέσω της οποίας παρουσιάστηκαν δέκα επιλεγμένα έργα ζωγραφικής (erga.zip) στους συμμετέχοντες.
Κατά τη διάρκεια της θέασης, οι εθελοντές είχαν τη δυνατότητα να επιλέξουν ένα ή περισσότερα συναισθήματα που θεωρούσαν ότι ένιωθαν για κάθε έργο. 
Παράλληλα, καταγράφονταν οι εκφράσεις του προσώπου τους μέσω συμβατικής κάμερας υπολογιστή, με χρήση της βιβλιοθήκης OpenCV, η οποία είχε ενσωματωθεί στον κώδικα της εφαρμογής.
Επιπλέον, πραγματοποιήθηκαν μετρήσεις ηλεκτροκαρδιογραφήματος (ECG) και ηλεκτροδερμικής δραστηριότητας (EDA), τα δεδομένα των οποίων καταγράφονταν μέσω της πλατφόρμας OpenSignals.
Για τη συγχρονισμένη συλλογή όλων των παραπάνω δεδομένων χρησιμοποιήθηκε το LabRecorder, με στόχο την αποθήκευσή τους σε ένα ενιαίο αρχείο μορφής XDF.
Η πειραματική εφαρμογή δημιουργούσε ξεχωριστά streams για τα συναισθήματα που ανέφεραν οι εθελοντές, δηλαδή τα reported emotions (EmotionStream) και για την καταγραφή εκφράσεων προσώπου (FaceStream), ενώ το OpenSignals παρήγαγε ταυτόχρονα stream για τα δεδομένα ECG και EDA.
Όλες οι ροές μεταδίδονταν μέσω του πρωτοκόλλου Lab Streaming Layer (LSL) και συλλέγονταν ταυτόχρονα από το LabRecorder, επιτρέποντας την αποθήκευσή τους σε κοινό χρονικό άξονα. 
Με τον τρόπο αυτό διασφαλίστηκε ο ακριβής χρονικός συγχρονισμός μεταξύ reported emotions, εκφράσεων προσώπου και βιοσημάτων.

Όσον αφορά την απάντηση στο πρώτο ερευρνητικό ερώτημα, αρχικά έγινε η ανάγνωση και οπτικοποίηση όλων των δεδομένων που συλλέχθηκαν.
Για να γίνει η ανάγνωση των δεδομένων χρησιμοποιήθηκε η εντολή load_xdf στο MATLAB.
Έπειτα, για να αναλύθουν τα δεδομένα απο τις εκφράσεις του προσώπου χρησιμοποιήθηκε η βιβλιόηθηκη deepface της python, μέσω της οποίας δημιουργήθηκε ένα script ανάλυσης των βίντεο (video_emotion.py)

1) Δεδομένα Ερωτηματολογίων : Data_Erotimatologia.zip
2) Δεδομένα Συναισθημάτων που επέλεξαν οι Εθελοντές απο την Εφαρμογή: ReportedEmotions_users.xlxs
3) Δεδομένα απο το DeepFace: DeepFace_Emotions.zip  (Στον συγκεκριμένο φάκελο υπάρχουν 23 excel αρχεία , ένα για κάθε χρήστη. Το κάθε αρχείο περιλαμβάνει 10 sheets , ένα για κάθε εικόνα. Οι στήλες των αρχείων αφορούν το frame ανάλυσης, το κυριάρχο συναίσθημα του χρήστη στο συγκεκριμένο frame και ύστερα ποσοστά απο το κάθε συναίσθημα)

Όσο για τα δεδομένα απο τους αισθητήρες παρουσιάζεται στα παρακάτω αρχεία ένα demo dataset (Του εθελοντή 1) :

4) Δεδομένα από τους αισθητήρες ECG και EDA: [ECG_EDA_user1.xlsx](https://github.com/user-attachments/files/25130824/ECG_EDA_user1.xlsx)
5) Plots απο τα σήματα ECG και EDA: 
[plots_user1.zip](https://github.com/user-attachments/files/25130828/plots_user1.zip)

Όλα τα δεδομένα βρίσκονται στο : https://drive.google.com/drive/folders/1UA2hp7aGDTdxPdnMZ9TfwmozNgscCDGq
